{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6234343a-981b-49de-b519-76ea85f45dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vqgan_with_pos_trial_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiscriminator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Discriminator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlpips\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LPIPS\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvqgan_with_pos_trial_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VQGAN\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, weights_init\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vqgan_with_pos_trial_v2'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57149259-d761-4652-8ea0-ad21274dec35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160883f8-1328-40e9-95ac-053009b950d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\"\"\"\n",
    "In GroupNorm, channels are divided into groups, and the mean and \n",
    "variance are calculated within each group across all instances in a \n",
    "batch. This makes GroupNorm's performance less dependent on the batch \n",
    "size, and it can maintain its effectiveness even with a batch size of 1. \n",
    "This can be particularly useful in applications where memory constraints \n",
    "limit the maximum batch size, such as high-resolution image processing.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class GroupNorm(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(GroupNorm, self).__init__()\n",
    "        \n",
    "        # num_grapups should be divisble by in_channels\n",
    "        # for example 32 is divisible by 128.\n",
    "        # 32 is divisible by 256\n",
    "        # 32 is divisibble by 5122\n",
    "        self.gn = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gn(x)\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            GroupNorm(in_channels),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            GroupNorm(out_channels),\n",
    "            Swish(),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)  # this layer is used to preserve depth which is used in return x + self.block(x)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.in_channels != self.out_channels:\n",
    "            return self.channel_up(x) + self.block(x)\n",
    "        else:\n",
    "            return x + self.block(x)\n",
    "\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpSampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2.0)\n",
    "        return self.conv(x)\n",
    "\n",
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        pe = torch.zeros(max_len, max_len, d_model)\n",
    "        position_i = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        position_j = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2.) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, :, 0::2] = torch.sin(position_i * div_term)\n",
    "        pe[:, :, 1::2] = torch.cos(position_j * div_term)\n",
    "\n",
    "        pe = pe.permute(2, 0, 1).unsqueeze(0)  # make shape [1, C, H, W]\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :, :x.size(2), :x.size(3)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownSampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pad = (0, 1, 0, 1)\n",
    "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
    "        return self.conv(x)\n",
    "\n",
    "# NonLocal NN: https://arxiv.org/pdf/1711.07971.pdf\n",
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        self.in_channels = channels\n",
    "\n",
    "        self.gn = GroupNorm(channels)\n",
    "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = self.gn(x)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        b, c, h, w = q.shape\n",
    "\n",
    "        q = q.reshape(b, c, h*w)\n",
    "        q = q.permute(0, 2, 1)\n",
    "        k = k.reshape(b, c, h*w)\n",
    "        v = v.reshape(b, c, h*w)\n",
    "\n",
    "        attn = torch.bmm(q, k)\n",
    "        attn = attn * (int(c)**(-0.5))\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        attn = attn.permute(0, 2, 1)\n",
    "\n",
    "        A = torch.bmm(v, attn)\n",
    "        A = A.reshape(b, c, h, w)\n",
    "\n",
    "        return x + A\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146da265-bc62-4e60-9b29-40bbba3c0b03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from helper import ResidualBlock, NonLocalBlock, DownSampleBlock, UpSampleBlock, GroupNorm, Swish\n",
    "\n",
    "# from PositionalEncoding2dJupyter import PositionalEncoding2D\n",
    "\n",
    "# from helperjupyter import ResidualBlock, NonLocalBlock, UpSampleBlock, GroupNorm, Swish,PositionalEncoding2D\n",
    "# Encoder block ==> Page 11/52 (pdf): https://arxiv.org/pdf/2012.09841.pdf\n",
    "\n",
    "\"\"\"_summary_\n",
    "\n",
    "Step-by-step breakdown of the Encoder module:\n",
    "\n",
    "Module 1 - Initial Convolution Layer\n",
    "\n",
    "Layer 1: Conv2d layer (input: image_channels, output: channels[0], kernel_size: 3, stride: 1, padding: 1)\n",
    "Module 2 - Residual Blocks and Non-local Blocks\n",
    "\n",
    "For each pair of consecutive channels in the channels list:\n",
    "Layer 2a: ResidualBlock (input: in_channels, output: out_channels)\n",
    "Layer 2b: ResidualBlock (input: out_channels, output: out_channels)\n",
    "If the current resolution is in attn_resolutions, add a NonLocalBlock (input: out_channels)\n",
    "If not at the last pair of consecutive channels, add a DownSampleBlock (input: out_channels)\n",
    "\n",
    "Module 3 - Final Layers\n",
    "\n",
    "Layer 3a: ResidualBlock (input: channels[-1], output: channels[-1])\n",
    "Layer 3b: NonLocalBlock (input: channels[-1])\n",
    "Layer 3c: ResidualBlock (input: channels[-1], output: channels[-1])\n",
    "Layer 3d: GroupNorm (input: channels[-1])\n",
    "Layer 3e: Swish activation\n",
    "Layer 3f: Conv2d layer (input: channels[-1], output: latent_dim, kernel_size: 3, stride: 1, padding: 1)\n",
    "The entire architecture is then built using a nn.Sequential layer containing all the layers from Module 1, Module 2, and Module 3.\n",
    "\n",
    "channels = [128, 128, 128, 256, 256, 512]\n",
    "There are 5 instances of Module 2 for the given channel list.\n",
    "\n",
    "Here's the breakdown:\n",
    "\n",
    "Module 2.1: Transition from channels[0] (128) to channels[1] (128)\n",
    "Module 2.2: Transition from channels[1] (128) to channels[2] (128)\n",
    "Module 2.3: Transition from channels[2] (128) to channels[3] (256)\n",
    "Module 2.4: Transition from channels[3] (256) to channels[4] (256)\n",
    "Module 2.5: Transition from channels[4] (256) to channels[5] (512)\n",
    "\n",
    "The NonLocalBlock is added based on the attn_resolutions list. \n",
    "In the given code, attn_resolutions = [16]. The purpose of using the NonLocalBlock is \n",
    "to model long-range dependencies within the input feature maps by capturing spatial relationships at specific resolutions.\n",
    "\n",
    "\n",
    "\n",
    "Returns:\n",
    "_type_: _description_\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         channels = [128, 128, 128, 256, 256, 512]\n",
    "#         attn_resolutions = [16]\n",
    "#         num_res_blocks = 2\n",
    "#         resolution = 256\n",
    "\n",
    "#         layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
    "#         layers.append(PositionalEncoding2D(channels[0], max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after first Conv2d\n",
    "        \n",
    "#         for i in range(len(channels)-1):\n",
    "#             in_channels = channels[i]\n",
    "#             out_channels = channels[i + 1]\n",
    "#             for j in range(num_res_blocks):\n",
    "#                 layers.append(ResidualBlock(in_channels, out_channels))\n",
    "#                 layers.append(PositionalEncoding2D(out_channels, max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after ResidualBlock\n",
    "#                 in_channels = out_channels\n",
    "#                 if resolution in attn_resolutions:\n",
    "#                     layers.append(NonLocalBlock(in_channels))\n",
    "#                     layers.append(PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after NonLocalBlock\n",
    "#             if i != len(channels)-2: \n",
    "#                 layers.append(DownSampleBlock(channels[i+1]))\n",
    "#                 resolution //= 2\n",
    "#         layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "#         layers.append(PositionalEncoding2D(channels[-1], max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after ResidualBlock\n",
    "#         layers.append(NonLocalBlock(channels[-1]))\n",
    "#         layers.append(PositionalEncoding2D(channels[-1], max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after NonLocalBlock\n",
    "#         layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "#         layers.append(PositionalEncoding2D(channels[-1], max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after ResidualBlock\n",
    "#         layers.append(GroupNorm(channels[-1]))\n",
    "#         layers.append(Swish())\n",
    "#         layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        channels = [128, 256, 512]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 1\n",
    "        resolution = 256\n",
    "\n",
    "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
    "        layers.append(PositionalEncoding2D(channels[0], max_len=512, dropout_prob=0.5))  # PositionalEncoding2D at the start\n",
    "        \n",
    "        for i in range(len(channels)-1):\n",
    "            in_channels = channels[i]\n",
    "            out_channels = channels[i + 1]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            # if i != len(channels)-1: \n",
    "            layers.append(DownSampleBlock(channels[i+1]))\n",
    "            resolution //= 2\n",
    "            \n",
    "            # Adding PositionalEncoding2D in the middle of the encoder\n",
    "            if i == len(channels) // 2:\n",
    "                layers.append(PositionalEncoding2D(channels[i+1], max_len=512, dropout_prob=0.5))  \n",
    "        layers.append(DownSampleBlock(channels[i+1]))\n",
    "        \n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(NonLocalBlock(channels[-1]))\n",
    "        layers.append(DownSampleBlock(channels[i+1]))\n",
    "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
    "        layers.append(GroupNorm(channels[-1]))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28958eb6-d506-48c6-8304-80f4688f5cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# # Your classes here: PositionalEncoding2D, ResidualBlock, NonLocalBlock, DownSampleBlock, GroupNorm, Swish\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self, latent_dim, image_channels, device):\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.image_channels = image_channels\n",
    "#         self.device = device\n",
    "# # \n",
    "# # class Encoder(nn.Module):\n",
    "#     # your Encoder code\n",
    "\n",
    "# # Set up the arguments\n",
    "# args = Args(latent_dim=256, image_channels=3, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# # Initialize the Encoder\n",
    "# encoder = Encoder(args)\n",
    "\n",
    "# # Create a random tensor of size (batch_size, channels, height, width)\n",
    "# x = torch.randn(5, 3, 256, 256)\n",
    "\n",
    "# # Pass the tensor through the encoder\n",
    "# try:\n",
    "#     y = encoder(x)\n",
    "#     print(\"Encoder executed successfully. Output shape:\", y.shape)\n",
    "# except IndexError as e:\n",
    "#     print(\"Error occurred during execution of encoder:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6258e726-91e7-40bf-bd7d-6edf26fa6407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, args):\n",
    "#         super(Decoder, self).__init__()\n",
    "#         channels = [512, 256, 256, 128, 128]\n",
    "#         attn_resolutions = [16]\n",
    "#         num_res_blocks = 3\n",
    "#         resolution = 16\n",
    "\n",
    "#         in_channels = channels[0]\n",
    "#         # first 4 layers of the decoder. \n",
    "#         layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
    "#                   PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5),  # PositionalEncoding2D after first Conv2d\n",
    "#                   ResidualBlock(in_channels, in_channels),\n",
    "#                   PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5),  # PositionalEncoding2D after ResidualBlock\n",
    "#                   NonLocalBlock(in_channels),\n",
    "#                   PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5),  # PositionalEncoding2D after NonLocalBlock\n",
    "#                   ResidualBlock(in_channels, in_channels),\n",
    "#                   PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5)]  # PositionalEncoding2D after ResidualBlock\n",
    "\n",
    "#         # all other layers. \n",
    "#         for i in range(len(channels)):\n",
    "#             out_channels = channels[i]\n",
    "#             for j in range(num_res_blocks):\n",
    "#                 layers.append(ResidualBlock(in_channels, out_channels))\n",
    "#                 layers.append(PositionalEncoding2D(out_channels, max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after ResidualBlock\n",
    "#                 in_channels = out_channels\n",
    "#                 if resolution in attn_resolutions:\n",
    "#                     layers.append(NonLocalBlock(in_channels))\n",
    "#                     layers.append(PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5))  # PositionalEncoding2D after NonLocalBlock\n",
    "#             if i != 0:\n",
    "#                 layers.append(UpSampleBlock(in_channels))\n",
    "#                 resolution *= 2\n",
    "\n",
    "#         layers.append(GroupNorm(in_channels))\n",
    "#         layers.append(Swish())\n",
    "#         layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
    "#         self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "from torchinfo import summary\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "        channels = [512, 256, 128]\n",
    "        attn_resolutions = [16]\n",
    "        num_res_blocks = 1\n",
    "        resolution = 16\n",
    "\n",
    "        in_channels = channels[0]\n",
    "        # first 4 layers of the decoder. \n",
    "        layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
    "                  PositionalEncoding2D(in_channels, max_len=512, dropout_prob=0.5)]  # PositionalEncoding2D after first Conv2d\n",
    "\n",
    "        # Adding Residual Blocks, NonLocalBlock, and UpSampleBlocks\n",
    "        for i in range(len(channels)):\n",
    "            out_channels = channels[i]\n",
    "            for j in range(num_res_blocks):\n",
    "                layers.append(ResidualBlock(in_channels, out_channels))\n",
    "                if i == len(channels) // 2 and j == num_res_blocks // 2:  # Approximately the middle of the network\n",
    "                    layers.append(PositionalEncoding2D(out_channels, max_len=512, dropout_prob=0.5))  # PositionalEncoding2D\n",
    "                in_channels = out_channels\n",
    "                if resolution in attn_resolutions:\n",
    "                    layers.append(NonLocalBlock(in_channels))\n",
    "            # if i != 0:\n",
    "            layers.append(UpSampleBlock(in_channels))\n",
    "            resolution *= 2\n",
    "\n",
    "        layers.append(UpSampleBlock(in_channels))\n",
    "        layers.append(GroupNorm(in_channels))\n",
    "        layers.append(Swish())\n",
    "        layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24468d54-c379-4814-94ca-e68e1a31c8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.latent_dim = 256\n",
    "#         self.image_channels = 3\n",
    "#         self.image_size = 256\n",
    "#         self.batch_size = 5\n",
    "\n",
    "# args = Args()\n",
    "# # decoder = Decoder(args)\n",
    "\n",
    "# # # Create a random tensor with shape (batch_size, latent_dim, height, width)\n",
    "# # x = torch.randn(1, args.latent_dim, 16, 16)\n",
    "# # output = decoder(x)\n",
    "\n",
    "# # print(output.shape)  # Should print torch.Size([1\n",
    "\n",
    "# # if __name__ == '__main__':\n",
    "    \n",
    "# #     parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "# #     parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
    "# #     parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
    "# #     parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
    "# #     parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
    "# #     parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
    "# #     parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
    "# #     parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
    "# #     parser.add_argument('--batch-size', type=int, default=3, help='Input batch size for training (default: 6)')\n",
    "# #     parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train (default: 50)')\n",
    "# #     parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
    "# #     parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
    "# #     parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
    "# #     parser.add_argument('--disc-start', type=int, default=100, help='When to start the discriminator (default: 0)')\n",
    "# #     parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
    "# #     parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
    "# #     parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
    "    \n",
    "# #     parser.add_argument('--checkpoint-path', type=str, default=None, help='Path to the checkpoint to resume training from (default: None)')\n",
    "# #     parser.add_argument('--checkpoint-vq-opt-path', type=str, default=None, help='Path to the VQ optimizer checkpoint to resume training from (default: None)')\n",
    "# #     parser.add_argument('--checkpoint-disc-opt-path', type=str, default=None, help='Path to the discriminator optimizer checkpoint to resume training from (default: None)')\n",
    "\n",
    "# #     args = parser.parse_args()\n",
    "#     # Create random input tensor\n",
    "#     # batch_size = 32\n",
    "#     # image_channels = 3\n",
    "#     # height = width = 256\n",
    "# input_tensor = torch.randn(args.batch_size, args.image_channels, args.image_size, args.image_size)\n",
    "\n",
    "# # # Create an instance of Encoder\n",
    "# # latent_dim = 256  # define this as per your requirement\n",
    "# decoder = Decoder(args)\n",
    "\n",
    "# # # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "# random_input_image = (5, 256,16, 16)\n",
    "# summary(model=decoder,\n",
    "#         input_size = random_input_image, \n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e88f571-d253-473b-8957-d11b57f24d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from encoder_with_positional_encoding_v2 import Encoder\n",
    "# from decoder_with_positional_encoding_v2 import Decoder\n",
    "from codebook import Codebook\n",
    "import math\n",
    "\n",
    "class VQGAN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VQGAN, self).__init__()\n",
    "        self.encoder = Encoder(args).to(device=args.device)\n",
    "        self.decoder = Decoder(args).to(device=args.device)\n",
    "        self.codebook = Codebook(args).to(device=args.device)       \n",
    "        \n",
    "        # original vq-gan has prequantization and post quantization convolution layers.\n",
    "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
    "\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        # print(\"encoded_images.shape\", encoded_images.shape)\n",
    "        # pre quantisation layer. \n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        \n",
    "        \n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        \n",
    "        # post quantization layer.\n",
    "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
    "        \n",
    "        # print(\"post_quant_conv_mapping.shape\", post_quant_conv_mapping.shape)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "        # print(\"decoded_imagess.shape\", decoded_images.shape)\n",
    "        \n",
    "        return decoded_images, codebook_indices, q_loss\n",
    "    \n",
    "\n",
    "    # these functions will be used by the transformers separately.. \n",
    "    def encode(self, imgs):\n",
    "        encoded_images = self.encoder(imgs)\n",
    "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
    "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
    "        return codebook_mapping, codebook_indices, q_loss\n",
    "\n",
    "    def decode(self, z):\n",
    "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
    "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
    "        return decoded_images\n",
    "\n",
    "    # variable lambda is the weighting factor, between vq-vae loss (or perceptual loss)\n",
    "    # and the GAN loss..  equation 7 in the vq-gan paper\n",
    "    # G stands for decoder and L stands for the last layer of decoder. \n",
    "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
    "        \n",
    "        # last layer and its weight\n",
    "        last_layer = self.decoder.model[-1]\n",
    "        last_layer_weight = last_layer.weight\n",
    "        \n",
    "        # gradients for both percetpaul and GAN loss. \n",
    "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
    "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
    "\n",
    "        # clipping lambda value between 0 and 10K and then returning lambda. \n",
    "        λ = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
    "        λ = torch.clamp(λ, 0, 1e4).detach()\n",
    "        return 0.8 * λ\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    The idea behind starting the discriminator later in the training process is to \n",
    "    give the generator (in this case, the VQGAN's encoder-decoder architecture) some \n",
    "    time to learn to reconstruct images before introducing the adversarial training \n",
    "    aspect. This strategy can help stabilize the training process and improve convergence.\n",
    "\n",
    "    In the initial phase of training, the generator learns to reconstruct images \n",
    "    without the pressure of fooling the discriminator. When the discriminator is \n",
    "    introduced later, the generator has already learned a decent representation of\n",
    "    the data, which helps it better cope with the adversarial training aspect. This can\n",
    "    lead to better performance and stability during training.\n",
    "    \n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
    "        if i < threshold:\n",
    "            disc_factor = value  # disc_factor is the discriminator weight.. \n",
    "        return disc_factor\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4922f9-4dfd-431e-8844-ec0b56d7541b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/verma.lu/.conda/envs/pytorch_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/verma.lu/.conda/envs/pytorch_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=1.14, VQ_Loss=2.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=1.1, VQ_Loss=1.89] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=1.08, VQ_Loss=1.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=1.04, VQ_Loss=0.623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=1.05, VQ_Loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=1.05, VQ_Loss=0.445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=1.04, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=1.03, VQ_Loss=0.428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=1, VQ_Loss=0.337]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.987, VQ_Loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.976, VQ_Loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.977, VQ_Loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.972, VQ_Loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.962, VQ_Loss=0.269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.955, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.971, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.94, VQ_Loss=0.258] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.939, VQ_Loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.914, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.873, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.912, VQ_Loss=0.292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.83, VQ_Loss=0.349] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.813, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.779, VQ_Loss=0.308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.712, VQ_Loss=0.417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.675, VQ_Loss=0.556]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.667, VQ_Loss=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.654, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.606, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.597, VQ_Loss=0.432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.533, VQ_Loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.487, VQ_Loss=0.481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.534, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.394, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.399, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.316, VQ_Loss=0.627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.311, VQ_Loss=0.472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.27, VQ_Loss=0.388] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.274, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.245, VQ_Loss=0.502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.244, VQ_Loss=0.482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.178, VQ_Loss=0.642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.257, VQ_Loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.2, VQ_Loss=0.415]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.151, VQ_Loss=0.343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.132, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.145, VQ_Loss=0.511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.126, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.083, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.089, VQ_Loss=0.556]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.076, VQ_Loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.081, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.075, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.117, VQ_Loss=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.064, VQ_Loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.077, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.044, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.054, VQ_Loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.055, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.26] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.334] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.336] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.237] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.281] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.182] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.185] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.062, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.22]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.19]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.219] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.091, VQ_Loss=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.193] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.239]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.253] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.258] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.261] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.399] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.077, VQ_Loss=0.581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.65] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.08, VQ_Loss=0.407] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.35] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0.091, VQ_Loss=0.496]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.422] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.46]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.423]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.489] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.065, VQ_Loss=0.676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.72] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:29<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 1: VQ Loss=0.7244212601866041, GAN Loss=0.010541149415075779\n",
      "Training losses after epoch 1: VQ Loss=0.34201622606327237, GAN Loss=0.12324958651264666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.76]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.671]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.002, VQ_Loss=1.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.009, VQ_Loss=1.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.037, VQ_Loss=1.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.008, VQ_Loss=1.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.041, VQ_Loss=1.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.112, VQ_Loss=0.989]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.005, VQ_Loss=1.02] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.934]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.951]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.004, VQ_Loss=1.01] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.005, VQ_Loss=1.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.034, VQ_Loss=1.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.071, VQ_Loss=1.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.945] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=1.02]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.005, VQ_Loss=1.04] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.026, VQ_Loss=1.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.012, VQ_Loss=1.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.093, VQ_Loss=1.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.01, VQ_Loss=1.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.012, VQ_Loss=1.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.028, VQ_Loss=1.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.02, VQ_Loss=1.06] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.003, VQ_Loss=1.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.07] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=1.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.014, VQ_Loss=1.07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.01, VQ_Loss=1.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.006, VQ_Loss=1.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.028, VQ_Loss=1.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.98] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.031, VQ_Loss=0.931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.002, VQ_Loss=1.06] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.004, VQ_Loss=1.04] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.017, VQ_Loss=1.05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.993]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.006, VQ_Loss=1.03] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.023, VQ_Loss=1.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.894]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.012, VQ_Loss=1.08] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.004, VQ_Loss=1.01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.036, VQ_Loss=1.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.057, VQ_Loss=1.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.008, VQ_Loss=1.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.07, VQ_Loss=0.961]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.045, VQ_Loss=1.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.851]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.994]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.854]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.739]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.788] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.921]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.899]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.086, VQ_Loss=0.959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=1.08] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.01, VQ_Loss=1.02]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.728]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.073, VQ_Loss=0.69] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.739]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.67] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.836]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.679] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.918]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.075, VQ_Loss=0.962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.007, VQ_Loss=1]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.859]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.883]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.948] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.831]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.024, VQ_Loss=1.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.84] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.795]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.78] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.969]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.818] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.061, VQ_Loss=0.853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.919]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.796]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.692] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.2, VQ_Loss=0.77]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.743]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.075, VQ_Loss=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.046, VQ_Loss=0.677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.848] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.597]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.719] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.105, VQ_Loss=0.649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.595]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.61] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.539]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.481]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.539]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.499]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.496]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.573]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.638]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.048, VQ_Loss=0.574]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.749]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.61]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.105, VQ_Loss=0.734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.51] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.573]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.534]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.457]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.629] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.589]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.065, VQ_Loss=0.588]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.607]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.568] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.57]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.104, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.461]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.53] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.52]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.541]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.39]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.389]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.516]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.243, VQ_Loss=0.509]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0.112, VQ_Loss=0.527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.542]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.058, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.553]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.509]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.186, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.151, VQ_Loss=0.436]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.516]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.583]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.571]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0.048, VQ_Loss=0.51] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.57] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.517]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.449] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:53<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:29<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 2: VQ Loss=0.42942060657909936, GAN Loss=0.00158191335549418\n",
      "Training losses after epoch 2: VQ Loss=0.7377792216511286, GAN Loss=0.017293740321976517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.38]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.43] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.416]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.397]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.468]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.597]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.411]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.464]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.128, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.466]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.433]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.398]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.535]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.139, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.693]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.801]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.744]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.539]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.563]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.324, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.328, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.511]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.529]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.511]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.518]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.494]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.404]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.424]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.458]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.484]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.532]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.425]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.374] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.409]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.411]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.369]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.517]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.426]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.378]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.518]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.574]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.061, VQ_Loss=0.535]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.083, VQ_Loss=0.586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.224, VQ_Loss=0.537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.203, VQ_Loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.445]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.417]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.41]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.494]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.423]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.448]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.117, VQ_Loss=0.545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.43]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.078, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.446]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.494]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.388]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.44] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.315]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.487]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.173, VQ_Loss=0.858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.543]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.484]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.425]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.466]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.366]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.531]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.178, VQ_Loss=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.538]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.51] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.421]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.432]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.336]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.333]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.076, VQ_Loss=0.381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.337]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.286]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.307]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.259] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.339]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.36]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.33] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.375]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.079, VQ_Loss=0.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0.037, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.401]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.583]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.562]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.138, VQ_Loss=0.433]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.441]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.411]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.314]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.431]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.462]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.297]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 3: VQ Loss=0.34705141953059604, GAN Loss=0.006544915435764815\n",
      "Training losses after epoch 3: VQ Loss=0.42749744071703566, GAN Loss=0.013111034839785318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.334]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.333]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.362]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.421]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.116, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.254, VQ_Loss=0.436]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.072, VQ_Loss=0.513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.36]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.485]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.453]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.297]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.369]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.26]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.265]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.459]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.527]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.28]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.489]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.314]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.334]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.534]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.52] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.448]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.53]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.463]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.115, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.232, VQ_Loss=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.523]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.114, VQ_Loss=0.477]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.48]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.456]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.433]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.402] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.387]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.289]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.181, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.047, VQ_Loss=0.308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.312]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.36] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.327]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.047, VQ_Loss=0.352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.459]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.3]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.302]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.366]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.334, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.387]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.357]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.05, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.575]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.47] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.363]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.442]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.337]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.515]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.416]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.089, VQ_Loss=0.396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.413]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.33]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.218]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.336]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.056, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.193]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.35] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.277]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.266]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.174, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.39]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.514]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.425]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.396]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.294]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.254]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.115, VQ_Loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.26]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.318]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.057, VQ_Loss=0.361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.334]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0.067, VQ_Loss=0.434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.392]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.435]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.352]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.417]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.436]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.068, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.197, VQ_Loss=0.584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.46]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.373]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.306]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.372]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:29<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 4: VQ Loss=0.35681910599981037, GAN Loss=0.16988496136452472\n",
      "Training losses after epoch 4: VQ Loss=0.37198965617703267, GAN Loss=0.011528578584350784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.174, VQ_Loss=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.331]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.147, VQ_Loss=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.371]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.362]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.432]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.417]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.062, VQ_Loss=0.534]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.499]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.284]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.378]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.393]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.423]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.11, VQ_Loss=0.301] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.541]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.464]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.413]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.466]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.333]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.299]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.52]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.47] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.443]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.34] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.247]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.429]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.419]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.293]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.331]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.059, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.359]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.332]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.091, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.674]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.37]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.347]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.373]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.523]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.468]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.463]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.416]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.582, VQ_Loss=0.569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.275]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.076, VQ_Loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.388]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.354, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.29]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.132, VQ_Loss=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.371]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.292]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.402]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.442]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.364]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.507] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.395]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.353] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.528]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.458]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.542]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.48]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.37]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.05, VQ_Loss=0.385] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.352]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.428]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.485]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.387]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.385]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.261]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.34] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.398]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.052, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.266, VQ_Loss=0.429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.465]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.553]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.527]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.47] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.058, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.249, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.34]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.463]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.427]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.278]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.38]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.55]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.352]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.423] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.33]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:53<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.324]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.332]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.052, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.578]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.714, VQ_Loss=0.413]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.556]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.386]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.445]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.232]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.29] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.103, VQ_Loss=0.593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.357]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.663]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.316, VQ_Loss=0.644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.619]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.414]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.29] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.435]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:28<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.435]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 6: VQ Loss=0.34609307433877673, GAN Loss=0.012904085360873653\n",
      "Training losses after epoch 6: VQ Loss=0.32781583660279057, GAN Loss=0.014993613942723246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.146, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.475, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.272]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.486]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.322]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.221]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.404]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.282]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.317]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.53] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.187, VQ_Loss=0.451]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.379]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.304]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.368]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.355]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.319]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.446]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.187, VQ_Loss=0.471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.061, VQ_Loss=0.682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.517]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.62]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.418]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.314]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.509]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.52]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.344]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.211, VQ_Loss=0.364]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.46] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.502]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.471]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.373]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.388]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.307]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.37]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.455]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.44] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.298]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.476]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.136, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.463]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.43] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.452]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.441]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.28] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.454]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.264, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.26]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.373]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.355]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.333]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.543]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.298]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.361]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.255]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.095, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=1.06, VQ_Loss=0.427] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.889]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.941]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.802]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.492]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.352]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.503]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.084, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.268]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.168, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.376]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.35]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:28<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 7: VQ Loss=0.38590985025678365, GAN Loss=0.13749397263330007\n",
      "Training losses after epoch 7: VQ Loss=0.3282229305317105, GAN Loss=0.012758996006765996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.427]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.344]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.295]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.078, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.38] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.089, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.335]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.094, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.247]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.337]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.422]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.378]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.308] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.435]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.187, VQ_Loss=0.475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.561]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.278]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.233]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.329]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.111, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.332, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.299]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0934]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0852]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.427, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.266]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.357]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.51]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.369]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.398]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.358]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.384]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.07, VQ_Loss=0.37]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.306]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.425]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.49, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.348]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.448]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.314]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.239]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.486]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.489]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.233]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.376]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.392]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.379]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.37]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.361, VQ_Loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.373, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.327]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.299]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.277, VQ_Loss=0.366]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0.062, VQ_Loss=0.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.3]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.452]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.456]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.533]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.305]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.502, VQ_Loss=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0.076, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:53<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.243]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:55<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 8: VQ Loss=0.25297383155141556, GAN Loss=0.00907834139718651\n",
      "Training losses after epoch 8: VQ Loss=0.2558561723039608, GAN Loss=0.013985185347570818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.332]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.313]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.345]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.089, VQ_Loss=0.522]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.363]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.339, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.457]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.486]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.278]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.574, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.305]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.35] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.243]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.457]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.242]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.046, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.514]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.4]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.084, VQ_Loss=0.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.293]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.393, VQ_Loss=0.314]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.44] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.266]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.106, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.281]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.308]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.29] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.279]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.048, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.429]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.42] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.401]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0945]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.069, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.25]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.522]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.232, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.108, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.28] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.269]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.28] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.241]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.305] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.33]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.334]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0.192, VQ_Loss=0.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.259, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.26] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0.19, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.254]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:52<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:31<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 9: VQ Loss=0.17781711667776107, GAN Loss=0.007609925305005812\n",
      "Training losses after epoch 9: VQ Loss=0.24295508598137383, GAN Loss=0.011498180695131777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.067, VQ_Loss=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.424]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.321]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.441]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.595]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.329]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.047, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.374]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.139, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.795]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.405]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.349]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.542]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.731, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.296]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.26] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.255]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.281]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.236]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.356]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.284]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.055, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.354]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.079, VQ_Loss=0.198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.394]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.383]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.077, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.254]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.345]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.552]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.419]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.485]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.288]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.385]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.335]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.044, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.425]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.32]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.374]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.085, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.24]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.328]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.568, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.118, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.325]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:10<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:14<?, ?it/s, GAN_Loss=0.059, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.368]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:15<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.352]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.256]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.242]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.281]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:25<?, ?it/s, GAN_Loss=0.164, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.351]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0946]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.324]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.302]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:39<?, ?it/s, GAN_Loss=0.074, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:40<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.173] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:42<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.264]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.326]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:46<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:48<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [04:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 10: VQ Loss=0.15344864960227694, GAN Loss=0.010714129712765239\n",
      "Training losses after epoch 10: VQ Loss=0.24757863267603947, GAN Loss=0.011430596684010537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.264, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.49, VQ_Loss=0.126] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.342]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.377]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.348]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0836]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.144] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.16, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.085, VQ_Loss=0.0864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.281]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.353]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.167, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.532]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.651, VQ_Loss=0.702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.62]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.256, VQ_Loss=0.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.441]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.346]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.293]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.25]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.304]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.332]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.142, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.313, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.544]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.555]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.242]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0975]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.276]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.309, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.243]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.061, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.05, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0874]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0965]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0984]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.411]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.318]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.312]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.106, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.121, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.212, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0991]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.074, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.198]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.277]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.344]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 11: VQ Loss=0.16221001339810234, GAN Loss=0.03076716572452694\n",
      "Training losses after epoch 11: VQ Loss=0.19082165231006315, GAN Loss=0.013344599477544638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.191] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.054, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.278]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.381, VQ_Loss=0.258]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.07, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.287, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.053, VQ_Loss=0.092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0785]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0966]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0912]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.081]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.49]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.292]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.099, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.403]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.257]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.338]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.098]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.269, VQ_Loss=0.183] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.282]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.242]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0973]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.15]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.245]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.253]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.238]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.246, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.166] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0944]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.271]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.444, VQ_Loss=0.0809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0716]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.219] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.284]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.181, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0929]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0958]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.251]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.334]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.095, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.512, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.164, VQ_Loss=0.236] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.164, VQ_Loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 12: VQ Loss=0.11265623963304928, GAN Loss=0.004351521303423007\n",
      "Training losses after epoch 12: VQ Loss=0.16060832802595112, GAN Loss=0.012387458594571321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.357, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.109, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.315]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.272]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.304]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.48] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.174, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0979]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.245]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.256]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.044, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.413]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.301]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.281]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.221]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.399, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.135, VQ_Loss=0.0969]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.198]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.289]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.206]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0894]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0731]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.205, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.351, VQ_Loss=0.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.2]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.13, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.095]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0896]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0989]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.086]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0594]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0954]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.113, VQ_Loss=0.094]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0945]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0886]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.221]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.314, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.213]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.262]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.45] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.215]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.0927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.406]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.283]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0961]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0998]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.375, VQ_Loss=0.0955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.049, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.259]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.408]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.214]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.25] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.044, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0.044, VQ_Loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 13: VQ Loss=0.3233829046998705, GAN Loss=0.050930453924232615\n",
      "Training losses after epoch 13: VQ Loss=0.15920018246679596, GAN Loss=0.011950193567549327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.31]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.197]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.185]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.086, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.0735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.209]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.207]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.153, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.072, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0751]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0756]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.0664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.073] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0946]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.166, VQ_Loss=0.0716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.303]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.291]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.208]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.217]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.206]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.23] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.198, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.059, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0822]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.54, VQ_Loss=0.152] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.166]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.098] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.149, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.131] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.05, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.062, VQ_Loss=0.188]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.198]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0852]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0779]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.079] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.283, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.841, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.068, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0934]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0805]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.186, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.112] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0922]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0722]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=1.02, VQ_Loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 14: VQ Loss=0.11940961512071746, GAN Loss=0.006350185367958667\n",
      "Training losses after epoch 14: VQ Loss=0.13965514796450484, GAN Loss=0.015411967059513379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.079]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.138] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0972]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.031, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.0952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.292]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.274]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.201]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0983]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.0749]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.119, VQ_Loss=0.268] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0921]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0887]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0951]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0904]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0826]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.1, VQ_Loss=0.0699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.501]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.115, VQ_Loss=0.215]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.27, VQ_Loss=0.168] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0734]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0914]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0749]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.284]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.198]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.0997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.057] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0757]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.08]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.156, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.275]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.086, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.092, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0916]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0949]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0759]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.088] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.0743]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.243] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.172]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.214, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.0817]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.0917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.088]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0869]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.153] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0859]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.116] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.127, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.134] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.099, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.439, VQ_Loss=0.0686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0857]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 15: VQ Loss=0.07966473113213267, GAN Loss=4.730977343569975e-06\n",
      "Training losses after epoch 15: VQ Loss=0.12754762032216646, GAN Loss=0.009126983549445097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0677]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0768]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.065]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.103, VQ_Loss=0.0901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.286]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.253]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0668]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.0834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.343]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.244]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.248]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.233]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.066] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.088]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.304]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.21, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.0686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0993]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0804]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.0649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.248, VQ_Loss=0.0905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.047, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.167] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.279]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.0823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0958]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0928]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0786]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0693]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.195, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.313, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.082]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0784]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0954]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.133]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.0752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0902]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0783]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0966]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.0882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0935] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.193]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.203]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0858]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0792]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0922]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.159, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.141, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.236]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.437]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.272, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.06] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0957]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0946]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0886]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0829]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.0579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.067]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0781]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 16: VQ Loss=0.09752975404262543, GAN Loss=0.002094169485254887\n",
      "Training losses after epoch 16: VQ Loss=0.1252460154996376, GAN Loss=0.007982012097596221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0983]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0961]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.083]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.056] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0965]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.0969]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.199, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.149] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.912, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.161, VQ_Loss=0.099]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0493]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0578]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0865]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0896]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.0794]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0958] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.221]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.227]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.095]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.277, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0709]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0574]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.056] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.0792]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.077] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0867]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.112] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0821]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.068, VQ_Loss=0.0978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.381]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.311]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.117, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.055, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.096]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0936]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.071, VQ_Loss=0.0836]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.064, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.055, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0768]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.0851]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.073]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.11, VQ_Loss=0.0698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.057, VQ_Loss=0.292]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0482]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.078]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0662]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.287]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.227, VQ_Loss=0.0853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.0661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0552]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0851]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.165]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0534]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0631]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0863]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.1] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.083]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.345] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.186]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0756]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.0949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.103, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.149]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0791]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 17: VQ Loss=0.10658297262021474, GAN Loss=0.006825132647656622\n",
      "Training losses after epoch 17: VQ Loss=0.11895537264770531, GAN Loss=0.010850553099138304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0585]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0971]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0832]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.341]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.076, VQ_Loss=0.181]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0925]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0972]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.078] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0576]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0946]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0897]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0794]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.064, VQ_Loss=0.0449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.078, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0964]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0976]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.08]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0921]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0908]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0984]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0975]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0929]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0784]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0804]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0956]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.054, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.675, VQ_Loss=0.053]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0826]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0728]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0668]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0546]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0991]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0901]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.289, VQ_Loss=0.0763]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.415, VQ_Loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0934]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0863]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0858]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.059, VQ_Loss=0.0795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.094]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0887]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0748]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0623]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0699]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.093] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0897]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0954]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.054, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0678]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0.031, VQ_Loss=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.211]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.138]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.123]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0777]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.058, VQ_Loss=0.0843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0648]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0934]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0993]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0525]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0575]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.164]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.08, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.16] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.212, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.085] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0928]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0983]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.067, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.195]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.191]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0.242, VQ_Loss=0.0651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0817]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 18: VQ Loss=0.2348177039197513, GAN Loss=4.610525920725195e-05\n",
      "Training losses after epoch 18: VQ Loss=0.10914699498701978, GAN Loss=0.0102810493227472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.261]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.213] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0899]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0999]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0858]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0644]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.216]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.122, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.073, VQ_Loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.273]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.219]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0928]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0601]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.281, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.098] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0966]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0914]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0783]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0942]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0914]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0546]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.103]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.116, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.071]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0853]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.047]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.0692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.34]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0984]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0918]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0709]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.042, VQ_Loss=0.0694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.145] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0888]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0804]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0565]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.075] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0987]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0612]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.148]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0944]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.189, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.082, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.322, VQ_Loss=0.187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0706]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0935]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0589]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0887]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0914]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.065]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.09] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0652]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0823]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.176, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.142]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.084, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.079, VQ_Loss=0.204]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.17] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0605]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0466]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0796]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.075] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.064] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0874]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.058] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0878]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.0723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0827]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.202] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.195] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.175]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0845]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0997]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.057, VQ_Loss=0.0659]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.077, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.196] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.073, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0814]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.081] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 19: VQ Loss=0.11374009810388089, GAN Loss=0.0011554177391043855\n",
      "Training losses after epoch 19: VQ Loss=0.10464012289187724, GAN Loss=0.007356546838790026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0792]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0848]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0841]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0767]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0733]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.187]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.094] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.145]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0737]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.0584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.099] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.173]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0777]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0692]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.072, VQ_Loss=0.0526]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.0913]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.228]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0896]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0724]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0964]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.184] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.268]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0804]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0896] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.225, VQ_Loss=0.14] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.081]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.182]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0936]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0507]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.082]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0573]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0777]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.091] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0772]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.0775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.067, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0911]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0674]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0993]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0869]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0795]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0526]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0819]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0733]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0925]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0.066, VQ_Loss=0.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0646]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0834]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0612]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0734]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0967]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.051] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0603]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.099]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0838]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0723]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0976]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.077]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0863]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.034, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0.022, VQ_Loss=0.194]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.123, VQ_Loss=0.232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0838]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0973]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.098] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0821]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.038, VQ_Loss=0.0506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.063, VQ_Loss=0.212]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.097]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0589]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0614]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0945]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.046, VQ_Loss=0.0714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0799]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0542]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0902]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0455]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.092] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0599]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0901]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0826]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0505]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.069]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.082, VQ_Loss=0.0596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.205]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.367]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.267]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.309]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.0566]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0799]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0949]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0495]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0692]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.062]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0975]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.069, VQ_Loss=0.0646]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0944]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 20: VQ Loss=0.10205713148627962, GAN Loss=0.001684318938474202\n",
      "Training losses after epoch 20: VQ Loss=0.09909835076492643, GAN Loss=0.00501920250150758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.092]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.063] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0509]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0556]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0843]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0884]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0763]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0872]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.451, VQ_Loss=0.0735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.085]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.0478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.083]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0803]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0903]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0808]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.099]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0761]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0522]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0529]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0534]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.051, VQ_Loss=0.0578]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.223, VQ_Loss=0.0712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.192]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.229]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.215, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.087]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0791]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0523]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0755]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0433]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0536]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.072] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.071] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0574]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0772]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0691]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0942]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0623]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0826]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0841]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0931]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.097]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0635]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.119, VQ_Loss=0.0676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0632]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0621]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0944]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.041]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.069] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0897]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0781]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.088]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.072]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.043, VQ_Loss=0.0484]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.145] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.135] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0887]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0939]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0961]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0.033, VQ_Loss=0.0459]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0589]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.062] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.0583]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0891]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0382]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.068] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0432]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0638]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0748]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0583]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.086] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0989]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0408]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0759]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0446]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0843]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0668]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.081] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.091] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0788]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0834]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0.379, VQ_Loss=0.0599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0714]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0734]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0572]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0964]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0562]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0883]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0709]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0604]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.081]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0548]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0518]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0777]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.075]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0901]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0955]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0983]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0857]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.105, VQ_Loss=0.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.171]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0.036, VQ_Loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.561, VQ_Loss=0.0689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0828]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0931]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.096] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.048, VQ_Loss=0.0548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0843]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0599]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0.121, VQ_Loss=0.0524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.0723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0828]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0362]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0661]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 21: VQ Loss=0.10851810233933586, GAN Loss=0.0005690401433996937\n",
      "Training losses after epoch 21: VQ Loss=0.08739550539640466, GAN Loss=0.00921331603522839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.132]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.017, VQ_Loss=0.0805]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0.158, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.084]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.037, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0843]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0799]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.073]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.082] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0954]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.086, VQ_Loss=0.152] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0668]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.068]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0967]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.014, VQ_Loss=0.0618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0644]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.072] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.08]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0657]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.079] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0811]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0765]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.133] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0972]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0821]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.024, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.161]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.217]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.14]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.069, VQ_Loss=0.057]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0574]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.0734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0685]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0778]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.097]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.076]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0662]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0709]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.141]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0849]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.03, VQ_Loss=0.0656]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0755]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.091]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0916]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0732]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.031, VQ_Loss=0.0571]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0785]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0663]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0937]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0917]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0573]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0605]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0891]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.149]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0941]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0915]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.0893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0565]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.0553]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.167]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.15] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0.067, VQ_Loss=0.107]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.027, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.168]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.155]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.099] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0557]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0418]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0779]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0891]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0439]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0591]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.054] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0503]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0912]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.031]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0793]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.091]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0688]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.093] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0909]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=1.14, VQ_Loss=0.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.235] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.054] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0749]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0589]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0976]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0465]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.055]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0547]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0634]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0624]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.02, VQ_Loss=0.0532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0462]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0766]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0757]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0722]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0819]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.06, VQ_Loss=0.0585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.058, VQ_Loss=0.051] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.196]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.428]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.121]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.146]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0801]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.085]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0743]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0613]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0867]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0682]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.127]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0911]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.0663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0583]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0651]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0731]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.069]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.016, VQ_Loss=0.0285]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.072, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.289]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.18] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.146] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.099]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0975]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0882]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.334, VQ_Loss=0.053]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.095, VQ_Loss=0.0479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0637]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0615]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0908]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.13]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0585]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0791]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0.026, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0814]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 22: VQ Loss=0.07683279370622975, GAN Loss=0.00022073850023604435\n",
      "Training losses after epoch 22: VQ Loss=0.0929885312087006, GAN Loss=0.009357572193072438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0406]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0473]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.121] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0903]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.055] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0555]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0764]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0753]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0768]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0352]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.073] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0748]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.101]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.166]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0.244, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0705]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0568]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0985]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0955]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0828]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0873]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0445]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0617]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0779]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0858]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0781]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.128, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.316, VQ_Loss=0.0632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.209]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0771]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0618]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0884]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0578]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0571]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0974]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.079]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0655]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.048] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0436]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0584]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0.079, VQ_Loss=0.0457]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0798]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0764]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0534]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0447]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0998]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0421]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0933]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.085] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0464]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.051]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0359]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0579]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.122, VQ_Loss=0.0338]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.316]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.188]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.029, VQ_Loss=0.0726]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0899]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0605]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0763]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0786]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.163]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.114]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0959]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0652]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0763]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.263]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0953]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0815]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0753]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0896]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.098] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0558]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0601]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0796]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.159]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0868]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0.228, VQ_Loss=0.112]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.0876]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.197]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0906]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0538]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0927]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0769]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0524]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0833]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0851]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0627]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0554]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0758]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0744]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0467]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0958]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0634]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0737]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.015, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0815]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0807]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0581]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.021, VQ_Loss=0.0398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0646]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0.041, VQ_Loss=0.0776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.025, VQ_Loss=0.0829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.199, VQ_Loss=0.0504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0986]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.079]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.0567]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0646]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0774]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0736]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0596]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0595]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0833]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0401]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0532]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0769]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0789]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0559]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0721]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0835]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0822]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0684]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0.074, VQ_Loss=0.0398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0588]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.074, VQ_Loss=0.0444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.223]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.323]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.258, VQ_Loss=0.147]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.039, VQ_Loss=0.0705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.249]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.179]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0925]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0483]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.126]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0719]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0894]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0458]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0486]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0554]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0487]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0576]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:09<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:37<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 23: VQ Loss=0.06295738238841295, GAN Loss=0.005925837943608972\n",
      "Training losses after epoch 23: VQ Loss=0.08893426669516949, GAN Loss=0.007480497568200998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0536]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0691]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0926]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.139]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0586]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0.028, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.152]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.142]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.189]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.119]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0399]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.103] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0475]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0846]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0696]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.069] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0723]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0909]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0578]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0927]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0885]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.105] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0748]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0.013, VQ_Loss=0.0688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0668]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0984]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.118] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0602]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0544]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.137] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0.011, VQ_Loss=0.0824]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0474]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0596]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0746]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0912]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0965]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0537]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.122] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0491]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0821]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0888]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0837]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0577]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.078] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.05, VQ_Loss=0.079] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0.012, VQ_Loss=0.132] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0719]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.091]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0859]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0569]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0854]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0787]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0745]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0786]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.08]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0775]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0679]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0.121, VQ_Loss=0.0472]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0.285, VQ_Loss=0.0449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.143]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.265]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.317]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.3]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.122]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0735]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0772]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0759]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.21]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0969]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0829]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0854]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0863]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.074]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0893]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0996]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.037, VQ_Loss=0.0639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:25<?, ?it/s, GAN_Loss=0.082, VQ_Loss=0.0747]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:26<?, ?it/s, GAN_Loss=0, VQ_Loss=0.174]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:27<?, ?it/s, GAN_Loss=0.057, VQ_Loss=0.143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.176]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0931]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.136] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.112] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0945]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0897]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0962]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0948]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.099] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0709]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0342]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0408]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0951]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0878]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0717]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:43<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0666]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0604]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.049]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:47<?, ?it/s, GAN_Loss=0.032, VQ_Loss=0.0531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0.007, VQ_Loss=0.0507]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0758]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.091] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:50<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0603]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:52<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0495]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.06]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0861]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0864]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0982]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0782]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0729]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:57<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0572]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0671]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0619]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0825]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0845]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0607]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0988]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.108]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0777]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0497]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:07<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.218, VQ_Loss=0.0391]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:08<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.0856] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0768]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.1]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.124] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.11] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:13<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0872]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0545]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0798]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:17<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0516]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0621]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0949]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0609]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0413]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0498]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0893]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0831]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:26<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:28<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0587]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:29<?, ?it/s, GAN_Loss=0, VQ_Loss=0.116] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0427]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0666]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:31<?, ?it/s, GAN_Loss=0.009, VQ_Loss=0.0434]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0576]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.156] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.129] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0828]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0898]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:37<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:38<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.065]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0677]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0513]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.064]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:42<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0519]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0565]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0424]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.076] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:46<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0923]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:48<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0963]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:49<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0816]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0595]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:50<?, ?it/s, GAN_Loss=0.019, VQ_Loss=0.0629]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.087, VQ_Loss=0.0662]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:51<?, ?it/s, GAN_Loss=0.099, VQ_Loss=0.0713]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:52<?, ?it/s, GAN_Loss=0, VQ_Loss=0.147]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.131]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:54<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.069]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0994]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.082] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:57<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0855]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.104] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [02:59<?, ?it/s, GAN_Loss=0.004, VQ_Loss=0.0512]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0779]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0755]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0932]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0564]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0842]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.057] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0652]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0886]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0761]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0626]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([2, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([2, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [03:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n",
      "Validation losses after epoch 24: VQ Loss=0.07447558508387633, GAN Loss=0.00033950318106365426\n",
      "Training losses after epoch 24: VQ Loss=0.0863228373304762, GAN Loss=0.004646759500038227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0938]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0512]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0642]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:04<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0738]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0552]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0725]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:07<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0407]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.07]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0566]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.045] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0372]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0415]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.053] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.115] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0575]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.123] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:16<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0541]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0725]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0522]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0911]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0752]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0771]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0919]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0818]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:22<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0598]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0496]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:24<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:25<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0571]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0606]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:26<?, ?it/s, GAN_Loss=0.04, VQ_Loss=0.121]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:27<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0749]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0889]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:28<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0866]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.005, VQ_Loss=0.0616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:29<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:30<?, ?it/s, GAN_Loss=0, VQ_Loss=0.106]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0611]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:31<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:32<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0576]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0.008, VQ_Loss=0.0597]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:33<?, ?it/s, GAN_Loss=0, VQ_Loss=0.102]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0727]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:34<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0739]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.044]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:35<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0588]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0528]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:36<?, ?it/s, GAN_Loss=0, VQ_Loss=0.101] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:37<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:38<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0481]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0539]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:39<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0856]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:40<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0696]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:41<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.0556]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:42<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0318]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0732]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:43<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:44<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0684]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:45<?, ?it/s, GAN_Loss=0.001, VQ_Loss=0.0673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:46<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0737]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.107] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:47<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0801]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:48<?, ?it/s, GAN_Loss=0.002, VQ_Loss=0.0511]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0991]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:49<?, ?it/s, GAN_Loss=0.01, VQ_Loss=0.0582]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:50<?, ?it/s, GAN_Loss=0.018, VQ_Loss=0.089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0473]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:51<?, ?it/s, GAN_Loss=0, VQ_Loss=0.144] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:52<?, ?it/s, GAN_Loss=0.188, VQ_Loss=0.118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0617]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:53<?, ?it/s, GAN_Loss=0, VQ_Loss=0.117] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:54<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0827]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:55<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:56<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0714]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.086, VQ_Loss=0.0586]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:57<?, ?it/s, GAN_Loss=0.045, VQ_Loss=0.0741]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.108]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:58<?, ?it/s, GAN_Loss=0, VQ_Loss=0.224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [00:59<?, ?it/s, GAN_Loss=0, VQ_Loss=0.128]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0924]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:00<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0548]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:01<?, ?it/s, GAN_Loss=0, VQ_Loss=0.113] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0.035, VQ_Loss=0.0742]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:02<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0862]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0951]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:03<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0561]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:04<?, ?it/s, GAN_Loss=0.006, VQ_Loss=0.134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0522]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:05<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:06<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0871]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.109] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:07<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0995]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:08<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0739]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.111] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:09<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0616]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:10<?, ?it/s, GAN_Loss=0, VQ_Loss=0.084] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:11<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0748]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0558]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:12<?, ?it/s, GAN_Loss=0, VQ_Loss=0.095] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:13<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0646]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:14<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0754]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:15<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:16<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0654]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:17<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0773]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:18<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:19<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0504]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:20<?, ?it/s, GAN_Loss=0, VQ_Loss=0.049] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0622]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:21<?, ?it/s, GAN_Loss=0.003, VQ_Loss=0.0521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:22<?, ?it/s, GAN_Loss=0, VQ_Loss=0.0724]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0, VQ_Loss=0.127] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape torch.Size([5, 3, 256, 256])\n",
      "decoded_images.shape torch.Size([5, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/297 [01:23<?, ?it/s, GAN_Loss=0.023, VQ_Loss=0.0726]"
     ]
    }
   ],
   "source": [
    "# python training_vqgan_v2.py --checkpoint-path \"C:/Users/luvve/VQGAN/checkpoints/vqgan_epoch_7.pt\"\n",
    "import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils\n",
    "from discriminator import Discriminator\n",
    "from lpips import LPIPS\n",
    "# from vqgan_with_pos_trial_v2 import VQGAN\n",
    "from utils import load_data, weights_init\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, main_directory, transform=None):\n",
    "        self.main_directory = main_directory\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(main_directory) if os.path.isfile(os.path.join(main_directory, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.main_directory, self.image_files[idx])\n",
    "        \n",
    "        # # Skip directories\n",
    "        if os.path.isdir(img_path):\n",
    "            return None\n",
    "\n",
    "    \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "class TrainVQGAN:\n",
    "    def __init__(self, args):\n",
    "        self.vqgan = VQGAN(args).to(device=args.device)\n",
    "        self.train_loader = self.load_data_2(args.train_folder, args.batch_size)\n",
    "        self.valid_loader = self.load_data_2(args.valid_folder, args.batch_size)\n",
    "\n",
    "        self.discriminator = Discriminator(args).to(device=args.device)\n",
    "        self.discriminator.apply(weights_init)\n",
    "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
    "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
    "        self.resumed_epoch = 0\n",
    "        if args.checkpoint_path is not None:\n",
    "            self.vqgan.load_state_dict(torch.load(args.checkpoint_path))\n",
    "            resumed_epoch = int(args.checkpoint_path.split('_')[-1].split('.')[0])\n",
    "            self.resumed_epoch = resumed_epoch\n",
    "            print(f\"Resuming training from checkpoint: {args.checkpoint_path}\")\n",
    "            print(f\"Resuming from epoch {resumed_epoch}\")\n",
    "\n",
    "        if args.checkpoint_vq_opt_path is not None and args.checkpoint_disc_opt_path is not None:\n",
    "            self.opt_vq.load_state_dict(torch.load(args.checkpoint_vq_opt_path))\n",
    "            self.opt_disc.load_state_dict(torch.load(args.checkpoint_disc_opt_path))\n",
    "            \n",
    "        self.prepare_training()\n",
    "\n",
    "        self.train(args)\n",
    "\n",
    "    def load_data_2(self, folder, batch_size):\n",
    "        transform = Compose([Resize((args.image_size, args.image_size)), ToTensor()])\n",
    "        dataset = CustomImageDataset(folder, transform=transform)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self, args):\n",
    "        lr = args.learning_rate\n",
    "        # vq_vae part optimizer\n",
    "        opt_vq = torch.optim.Adam(\n",
    "            list(self.vqgan.encoder.parameters()) +\n",
    "            list(self.vqgan.decoder.parameters()) +\n",
    "            list(self.vqgan.codebook.parameters()) +\n",
    "            list(self.vqgan.quant_conv.parameters()) +\n",
    "            list(self.vqgan.post_quant_conv.parameters()),\n",
    "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
    "        )\n",
    "        # discriminator part. \n",
    "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
    "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
    "\n",
    "        return opt_vq, opt_disc\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_training():\n",
    "        os.makedirs(\"results_2700Images_with_pos_pruned_v1\", exist_ok=True)\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        folder_name = args.train_folder\n",
    "        # os.makedirs(os.path.join(\"results\", folder_name), exist_ok=True)\n",
    "\n",
    "    def train(self, args):\n",
    "        # train_dataset = load_data(args)\n",
    "        # steps_per_epoch = len(train_dataset)\n",
    "        tensorboard_log_dir = f'runs/lr={args.learning_rate}_with_pos_2700Imagespruned_v1_totalepochs={args.epochs}_perceptual_loss_factor={args.perceptual_loss_factor}_discfactor={args.disc_start}_codebook={args.num_codebook_vectors}_latentdim={args.latent_dim}_{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "        writer = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "        steps_per_epoch = len(self.train_loader)\n",
    "        # output_path = os.path.join(args.train_folder, \"results\")\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.resumed_epoch+1,args.epochs):\n",
    "            total_vq_loss = 0  # initialize total_vq_loss\n",
    "            total_gan_loss = 0  # initialize total_gan_loss\n",
    "            # The tqdm function is being used here to provide a progress bar for each epoch of training. \n",
    "            with tqdm(range(len(self.train_loader))) as pbar:\n",
    "                \n",
    "                # In this particular case, the zip function is used to iterate \n",
    "                # over both the progress bar and the training dataset simultaneously.\n",
    "                for i, imgs in enumerate(self.train_loader):\n",
    "                    # imgs.shape torch.Size([5, 3, 256, 256]) ==> first dimension is batch size(5)\n",
    "                    # remaining dimension is 3,256,256\n",
    "                    imgs = imgs.to(device=args.device)\n",
    "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
    "\n",
    "                    disc_real = self.discriminator(imgs)\n",
    "                    disc_fake = self.discriminator(decoded_images)\n",
    "\n",
    "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n",
    "                    \n",
    "                    print(\"imgs.shape\",imgs.shape)\n",
    "                    # decoded_images, _, q_loss = self.vqgan(imgs)\n",
    "                    print(\"decoded_images.shape\",decoded_images.shape)\n",
    "                \n",
    "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
    "                    rec_loss = torch.abs(imgs - decoded_images)\n",
    "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
    "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "                    g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "                    λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
    "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
    "\n",
    "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
    "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
    "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
    "\n",
    "                    # Backward loss.. \n",
    "                    \n",
    "                    self.opt_vq.zero_grad()\n",
    "                    vq_loss.backward(retain_graph=True)\n",
    "\n",
    "                    self.opt_disc.zero_grad()\n",
    "                    gan_loss.backward()\n",
    "\n",
    "                    self.opt_vq.step()\n",
    "                    self.opt_disc.step()\n",
    "\n",
    "                    # if i % 10 == 0:\n",
    "                    #     with torch.no_grad():\n",
    "                    #         real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
    "                    #         vutils.save_image(real_fake_images, os.path.join(\"results\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
    "                    total_vq_loss += vq_loss.item()\n",
    "                    total_gan_loss += gan_loss.item()\n",
    "                    \n",
    "                    pbar.set_postfix(\n",
    "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
    "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
    "                    )\n",
    "                    pbar.update(0)\n",
    "                \n",
    "                # assuming real_fake_images, epoch, i are defined\n",
    "                # and args is an instance of Args class with train_folder attribute defined\n",
    "                \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
    "                    vutils.save_image(real_fake_images, os.path.join(\"results_2700Images_with_pos_pruned_v1\", f\"{epoch}_{i}_{args.latent_dim}_{args.num_codebook_vectors}_{args.disc_start}_{args.epochs}.jpg\"), nrow=4)\n",
    "                    \n",
    "                avg_vq_loss_validation, avg_gan_loss_validation = self.validate(epoch)\n",
    "                # torch.save(self.vqgan.state_dict(), os.path.join(\"checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
    "                \n",
    "                avg_vq_loss = total_vq_loss / len(self.train_loader)\n",
    "                avg_gan_loss = total_gan_loss / len(self.train_loader)\n",
    "                print(f'Training losses after epoch {epoch}: VQ Loss={avg_vq_loss}, GAN Loss={avg_gan_loss}')\n",
    "\n",
    "                \n",
    "                # print(f'Training losses after epoch {epoch}: VQ Loss={avg_vq_loss}, GAN Loss={avg_gan_loss}')\n",
    "                # # Saving the optimizer state\n",
    "                # torch.save(self.opt_vq.state_dict(), os.path.join(\"checkpoints\", f\"opt_vq_epoch_{epoch}.pt\"))\n",
    "                # torch.save(self.opt_disc.state_dict(), os.path.join(\"checkpoints\", f\"opt_disc_epoch_{epoch}.pt\"))\n",
    "                # Log losses and metrics for each training iteration:\n",
    "            writer.add_scalar('Training VQ Loss', vq_loss.item(), epoch)\n",
    "            writer.add_scalar('Training GAN Loss', gan_loss.item(), epoch)\n",
    "\n",
    "            #  Log average losses and metrics for each epoch:\n",
    "            writer.add_scalar('Validation VQ Loss', avg_vq_loss_validation, epoch)\n",
    "            writer.add_scalar('Validation GAN Loss', avg_gan_loss_validation, epoch)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "    def validate(self, epoch):\n",
    "        with torch.enable_grad():\n",
    "            total_vq_loss = 0\n",
    "            total_gan_loss = 0\n",
    "            # for i, (imgs, _) in enumerate(self.valid_loader):\n",
    "            for i, imgs in enumerate(self.valid_loader):\n",
    "                imgs = imgs.to(device=args.device)\n",
    "                print(\"imgs.shape\",imgs.shape)\n",
    "                decoded_images, _, q_loss = self.vqgan(imgs)\n",
    "                print(\"decoded_images.shape\",decoded_images.shape)\n",
    "                disc_real = self.discriminator(imgs)\n",
    "                disc_fake = self.discriminator(decoded_images)\n",
    "                disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*len(self.valid_loader)+i, threshold=args.disc_start)\n",
    "\n",
    "                perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
    "                rec_loss = torch.abs(imgs - decoded_images)\n",
    "                perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
    "                perceptual_rec_loss = perceptual_rec_loss.mean()\n",
    "                g_loss = -torch.mean(disc_fake)\n",
    "\n",
    "                # with torch.enable_grad():  # enabling gradient computation temporarily\n",
    "                λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
    "                vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
    "                # total_vq_loss += vq_loss.item()\n",
    "\n",
    "                d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
    "                d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
    "                gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
    "                total_gan_loss += gan_loss.item()\n",
    "                total_vq_loss += vq_loss.item()\n",
    "                # total_gan_loss += gan_loss.item()\n",
    "                \n",
    "\n",
    "            avg_vq_loss_validation = total_vq_loss / len(self.valid_loader)\n",
    "            avg_gan_loss_validation = total_gan_loss / len(self.valid_loader)\n",
    "            print(f'Validation losses after epoch {epoch}: VQ Loss={avg_vq_loss_validation}, GAN Loss={avg_gan_loss_validation}')\n",
    "\n",
    "        return avg_vq_loss_validation, avg_gan_loss_validation    # # Log average losses and metrics for each epoch:\n",
    "    #     writer.add_scalar('Validation VQ Loss', avg_vq_loss, epoch)\n",
    "    #     writer.add_scalar('Validation GAN Loss', avg_gan_loss, epoch)    \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Just to show how you can use the arguments:\n",
    "# train_vqgan = TrainVQGAN(args)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self,train_folder_name,valid_folder_name, latent_dim,codebook,disc_value):\n",
    "        \n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_size = 256\n",
    "        self.num_codebook_vectors = codebook\n",
    "        self.beta = 0.25\n",
    "        self.image_channels = 3\n",
    "        self.dataset_path = '/data'\n",
    "        self.device = 'cuda'\n",
    "        self.batch_size = 5\n",
    "        self.epochs = 1000\n",
    "        self.learning_rate = 2.25e-05\n",
    "        self.beta1 = 0.5\n",
    "        self.beta2 = 0.9\n",
    "        self.disc_start = disc_value\n",
    "        self.disc_factor = 1.\n",
    "        self.rec_loss_factor = 1.\n",
    "        self.perceptual_loss_factor = 1\n",
    "        self.checkpoint_path = None\n",
    "        self.checkpoint_vq_opt_path = None\n",
    "        self.checkpoint_disc_opt_path = None\n",
    "        self.train_folder = os.path.join(os.getcwd(), train_folder_name)\n",
    "        self.valid_folder = os.path.join(os.getcwd(), valid_folder_name)\n",
    "        # Create a unique name for the tensorboard log directory\n",
    "        \n",
    "\n",
    "        # Define the folder names\n",
    "# train_folder_name = ['train_all_only_few','train_all', 'train_all_8K']\n",
    "# valid_folder_name = ['valid_all_only_few','valid_all', 'valid_all_8K']\n",
    "\n",
    "train_folder_name = ['train_all', 'train_all_8K']\n",
    "valid_folder_name = ['valid_all', 'valid_all_8K']\n",
    "\n",
    "codebook_vector = [8192]\n",
    "latent_dims = [256]\n",
    "disc_starts = [100]\n",
    "# perceptual_loss_factors = [0.5]\n",
    "# rec_loss_factors = [0.5]\n",
    "# for i in range(1,len(train_folder_name)):\n",
    "for i in range(1):\n",
    "    \n",
    "    for latent_dim in latent_dims:\n",
    "        for codebook in codebook_vector:\n",
    "            for disc_value in disc_starts:\n",
    "                args = Args(train_folder_name[i], valid_folder_name[i], latent_dim,codebook, disc_value)\n",
    "\n",
    "\n",
    "\n",
    "                # Just to show how you can use the arguments:\n",
    "                train_vqgan = TrainVQGAN(args)\n",
    "\n",
    "\n",
    "\n",
    "            # if __name__ == '__main__':\n",
    "            #     parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "            #     parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
    "            #     parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
    "            #     parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
    "            #     parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
    "            #     parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
    "            #     parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
    "            #     parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
    "            #     parser.add_argument('--batch-size', type=int, default=5, help='Input batch size for training (default: 6)')\n",
    "            #     parser.add_argument('--epochs', type=int, default=2, help='Number of epochs to train (default: 50)')\n",
    "            #     parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
    "            #     parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
    "            #     parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
    "            #     parser.add_argument('--disc-start', type=int, default=100, help='When to start the discriminator (default: 0)')\n",
    "            #     parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
    "            #     parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
    "            #     parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
    "\n",
    "            #     parser.add_argument('--checkpoint-path', type=str, default=None, help='Path to the checkpoint to resume training from (default: None)')\n",
    "            #     parser.add_argument('--checkpoint-vq-opt-path', type=str, default=None, help='Path to the VQ optimizer checkpoint to resume training from (default: None)')\n",
    "            #     parser.add_argument('--checkpoint-disc-opt-path', type=str, default=None, help='Path to the discriminator optimizer checkpoint to resume training from (default: None)')\n",
    "            #     #  add some new arguments\n",
    "            #     # parser.add_argument('--train-folder', type=str, default='/VQGANs/train_all/', help='Path to training data folder')\n",
    "            #     # parser.add_argument('--valid-folder', type=str, default='/VQGANs/valid_all/', help='Path to validation data folder')\n",
    "            # #\n",
    "            #     # parser.add_argument('--train-folder', type=str, default='/home/verma.lu/VQGANs/train_all/', help='Path to training data folder')\n",
    "            #     # parser.add_argument('--valid-folder', type=str, default='/home/verma.lu/VQGANs/valid_all/', help='Path to validation data folder')\n",
    "\n",
    "            #     parser.add_argument('--train-folder', type=str, default='./train_all/', help='Path to training data folder')\n",
    "            #     parser.add_argument('--valid-folder', type=str, default='./valid_all/', help='Path to validation data folder')\n",
    "            #     args = parser.parse_args()\n",
    "            #     args.checkpoint_path = None\n",
    "            #     args.checkpoint_disc_opt_path = None\n",
    "            #     args.checkpoint_vq_opt_path = None\n",
    "\n",
    "            #     # args.dataset_path = r\"C:\\Users\\luvve\\flowersSix\\jpg\"\n",
    "\n",
    "\n",
    "            #     # args.dataset_path = r\"C:\\Users\\luvve\\flowers\\jpg\"\n",
    "            #     # args.checkpoint_path = r\"C:\\Users\\luvve\\VQGAN\\checkpoints\\vqgan_epoch_61.pt\"\n",
    "            #     # args.checkpoint_vq_opt_path = r\"C:\\Users\\luvve\\VQGAN\\checkpoints\\opt_vq_epoch_61.pt\"\n",
    "            #     # args.checkpoint_disc_opt_path = r\"C:\\Users\\luvve\\VQGAN\\checkpoints\\opt_disc_epoch_61.pt\"\n",
    "\n",
    "            #     train_vqgan = TrainVQGAN(args)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380348c7-d09e-4db8-aef8-05d66d83adcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('results', 'zip', 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f5b32b-8ec7-4ade-9420-d119b61010ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/ (stored 0%)\n",
      "  adding: results/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: results/.ipynb_checkpoints/131_296_256_2048-checkpoint.jpg (deflated 1%)\n",
      "  adding: results/.ipynb_checkpoints/1_296_256_2048-checkpoint.jpg (deflated 7%)\n",
      "  adding: results/.ipynb_checkpoints/1_296_256_512-checkpoint.jpg (deflated 5%)\n",
      "  adding: results/.ipynb_checkpoints/23_296_256_512-checkpoint.jpg (deflated 1%)\n",
      "  adding: results/.ipynb_checkpoints/298_296_256_2048-checkpoint.jpg (deflated 1%)\n",
      "  adding: results/.ipynb_checkpoints/298_296_256_512-checkpoint.jpg (deflated 1%)\n",
      "  adding: results/.ipynb_checkpoints/299_296_256_2048-checkpoint.jpg (deflated 2%)\n",
      "  adding: results/.ipynb_checkpoints/299_296_256_512-checkpoint.jpg (deflated 1%)\n",
      "  adding: results/100_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/100_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/101_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/101_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/102_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/102_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/103_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/103_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/104_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/104_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/105_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/105_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/106_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/106_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/107_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/107_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/108_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/108_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/109_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/109_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/10_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/10_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/110_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/110_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/111_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/111_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/112_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/112_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/113_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/113_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/114_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/114_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/115_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/115_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/116_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/116_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/117_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/117_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/118_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/118_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/119_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/119_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/11_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/11_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/120_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/120_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/121_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/121_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/122_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/122_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/123_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/123_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/124_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/124_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/125_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/125_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/126_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/126_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/127_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/127_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/128_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/128_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/129_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/129_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/12_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/12_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/130_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/130_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/131_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/131_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/132_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/132_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/133_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/133_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/134_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/134_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/135_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/135_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/136_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/136_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/137_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/137_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/138_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/138_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/139_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/139_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/13_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/13_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/140_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/140_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/141_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/141_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/142_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/142_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/143_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/143_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/144_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/144_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/145_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/145_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/146_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/146_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/147_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/147_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/148_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/148_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/149_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/149_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/14_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/14_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/150_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/150_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/151_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/151_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/152_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/152_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/153_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/153_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/154_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/154_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/155_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/155_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/156_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/156_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/157_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/157_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/158_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/158_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/159_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/159_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/15_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/15_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/160_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/160_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/161_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/161_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/162_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/162_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/163_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/163_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/164_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/164_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/165_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/165_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/166_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/166_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/167_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/167_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/168_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/168_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/169_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/169_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/16_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/16_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/170_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/170_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/171_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/171_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/172_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/172_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/173_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/173_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/174_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/174_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/175_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/175_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/176_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/176_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/177_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/177_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/178_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/178_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/179_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/179_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/17_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/17_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/180_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/180_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/181_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/181_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/182_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/182_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/183_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/183_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/184_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/184_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/185_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/185_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/186_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/186_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/187_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/187_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/188_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/188_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/189_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/189_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/18_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/18_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/190_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/190_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/191_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/191_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/192_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/192_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/193_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/193_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/194_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/194_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/195_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/195_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/196_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/196_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/197_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/197_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/198_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/198_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/199_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/199_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/19_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/19_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/1_296_256_2048.jpg (deflated 7%)\n",
      "  adding: results/1_296_256_512.jpg (deflated 5%)\n",
      "  adding: results/200_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/200_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/201_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/201_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/202_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/202_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/203_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/203_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/204_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/204_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/205_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/205_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/206_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/206_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/207_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/207_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/208_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/208_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/209_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/209_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/20_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/20_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/210_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/210_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/211_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/211_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/212_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/212_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/213_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/213_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/214_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/214_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/215_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/215_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/216_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/216_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/217_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/217_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/218_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/218_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/219_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/219_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/21_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/21_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/220_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/220_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/221_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/221_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/222_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/222_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/223_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/223_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/224_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/224_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/225_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/225_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/226_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/226_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/227_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/227_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/228_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/228_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/229_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/229_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/22_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/22_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/230_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/230_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/231_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/231_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/232_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/232_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/233_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/233_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/234_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/234_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/235_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/235_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/236_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/236_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/237_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/237_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/238_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/238_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/239_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/239_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/23_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/23_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/240_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/240_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/241_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/241_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/242_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/242_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/243_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/243_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/244_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/244_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/245_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/245_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/246_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/246_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/247_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/247_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/248_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/248_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/249_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/249_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/24_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/24_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/250_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/250_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/251_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/251_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/252_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/252_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/253_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/253_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/254_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/254_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/255_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/255_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/256_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/256_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/257_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/257_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/258_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/258_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/259_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/259_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/25_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/25_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/260_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/260_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/261_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/261_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/262_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/262_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/263_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/263_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/264_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/264_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/265_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/265_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/266_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/266_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/267_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/267_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/268_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/268_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/269_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/269_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/26_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/26_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/270_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/270_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/271_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/271_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/272_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/272_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/273_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/273_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/274_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/274_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/275_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/275_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/276_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/276_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/277_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/277_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/278_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/278_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/279_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/279_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/27_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/27_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/280_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/280_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/281_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/281_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/282_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/282_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/283_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/283_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/284_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/284_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/285_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/285_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/286_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/286_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/287_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/287_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/288_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/288_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/289_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/289_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/28_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/28_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/290_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/290_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/291_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/291_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/292_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/292_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/293_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/293_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/294_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/294_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/295_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/295_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/296_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/296_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/297_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/297_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/298_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/298_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/299_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/299_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/29_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/29_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/2_296_256_2048.jpg (deflated 4%)\n",
      "  adding: results/2_296_256_512.jpg (deflated 4%)\n",
      "  adding: results/30_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/30_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/31_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/31_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/32_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/32_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/33_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/33_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/34_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/34_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/35_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/35_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/36_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/36_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/37_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/37_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/38_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/38_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/39_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/39_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/3_296_256_2048.jpg (deflated 4%)\n",
      "  adding: results/3_296_256_512.jpg (deflated 3%)\n",
      "  adding: results/40_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/40_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/41_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/41_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/42_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/42_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/43_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/43_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/44_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/44_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/45_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/45_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/46_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/46_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/47_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/47_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/48_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/48_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/49_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/49_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/4_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/4_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/50_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/50_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/51_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/51_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/52_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/52_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/53_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/53_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/54_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/54_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/55_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/55_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/56_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/56_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/57_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/57_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/58_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/58_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/59_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/59_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/5_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/5_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/60_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/60_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/61_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/61_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/62_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/62_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/63_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/63_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/64_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/64_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/65_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/65_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/66_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/66_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/67_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/67_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/68_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/68_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/69_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/69_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/6_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/6_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/70_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/70_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/71_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/71_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/72_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/72_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/73_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/73_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/74_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/74_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/75_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/75_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/76_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/76_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/77_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/77_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/78_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/78_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/79_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/79_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/7_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/7_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/80_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/80_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/81_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/81_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/82_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/82_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/83_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/83_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/84_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/84_296_256_512.jpg (deflated 2%)\n",
      "  adding: results/85_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/85_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/86_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/86_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/87_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/87_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/88_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/88_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/89_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/89_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/8_296_256_2048.jpg (deflated 3%)\n",
      "  adding: results/8_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/90_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/90_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/91_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/91_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/92_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/92_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/93_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/93_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/94_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/94_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/95_296_256_2048.jpg (deflated 2%)\n",
      "  adding: results/95_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/96_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/96_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/97_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/97_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/98_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/98_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/99_296_256_2048.jpg (deflated 1%)\n",
      "  adding: results/99_296_256_512.jpg (deflated 1%)\n",
      "  adding: results/9_296_256_2048.jpg (deflated 4%)\n",
      "  adding: results/9_296_256_512.jpg (deflated 2%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r results.zip results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886d6f7e-fbba-4285-842a-8e50a6de3593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install albumentations\n",
    "\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9d18aa8-f778-4863-98ef-2bffd400c52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/verma.lu/VQGANs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_path = os.getcwd()\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa3a01f-c45d-41f7-84f4-3c2b79eb3390",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 218\u001b[0m\n\u001b[1;32m    216\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--train-folder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_all/\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath to training data folder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    217\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--valid-folder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./valid_all/\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath to validation data folder\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m args\u001b[38;5;241m.\u001b[39mcheckpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    220\u001b[0m args\u001b[38;5;241m.\u001b[39mcheckpoint_disc_opt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/argparse.py:1828\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argv:\n\u001b[1;32m   1827\u001b[0m     msg \u001b[38;5;241m=\u001b[39m _(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrecognized arguments: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/argparse.py:2582\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_usage(_sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m   2581\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2582\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_env/lib/python3.9/argparse.py:2569\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message:\n\u001b[1;32m   2568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2569\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f907eb5-2cfa-426f-9651-46ab03a61a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
